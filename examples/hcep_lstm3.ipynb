{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"LSTM language model on text8.\n",
    "\n",
    "Default hyperparameters achieve ~78.4 NLL at epoch 50, ~76.1423 NLL at\n",
    "epoch 200; ~13s/epoch on Titan X (Pascal).\n",
    "\n",
    "Samples after 200 epochs:\n",
    "```\n",
    "e the classmaker was cut apart rome the charts sometimes known a\n",
    "hemical place baining examples of equipment accepted manner clas\n",
    "uetean meeting sought to exist as this waiting an excerpt for of\n",
    "erally enjoyed a film writer of unto one two volunteer humphrey\n",
    "y captured by the saughton river goodness where stones were nota\n",
    "```\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from edward.models import Categorical\n",
    "from edward.util import Progbar\n",
    "from observations import text8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/tmp/data\"\n",
    "log_dir = \"/tmp/log\"\n",
    "n_epoch = 200\n",
    "batch_size = 128\n",
    "hidden_size = 512\n",
    "timesteps = 64\n",
    "lr = 5e-3\n",
    "\n",
    "timestamp = datetime.strftime(datetime.utcnow(), \"%Y%m%d_%H%M%S\")\n",
    "hyperparam_str = '_'.join([\n",
    "    var + '_' + str(eval(var)).replace('.', '_')\n",
    "    for var in ['batch_size', 'hidden_size', 'timesteps', 'lr']])\n",
    "log_dir = os.path.join(log_dir, timestamp + '_' + hyperparam_str)\n",
    "if not os.path.exists(log_dir):\n",
    "  os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell(x, h, c, name=None, reuse=False):\n",
    "  \"\"\"LSTM returning hidden state and content cell at a specific timestep.\"\"\"\n",
    "  nin = x.shape[-1].value\n",
    "  nout = h.shape[-1].value\n",
    "  with tf.variable_scope(name, default_name=\"lstm\",\n",
    "                         values=[x, h, c], reuse=reuse):\n",
    "    wx = tf.get_variable(\"kernel/input\", [nin, nout * 4],\n",
    "                         dtype=tf.float32,\n",
    "                         initializer=tf.orthogonal_initializer(1.0))\n",
    "    wh = tf.get_variable(\"kernel/hidden\", [nout, nout * 4],\n",
    "                         dtype=tf.float32,\n",
    "                         initializer=tf.orthogonal_initializer(1.0))\n",
    "    b = tf.get_variable(\"bias\", [nout * 4],\n",
    "                        dtype=tf.float32,\n",
    "                        initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "  z = tf.matmul(x, wx) + tf.matmul(h, wh) + b\n",
    "  i, f, o, u = tf.split(z, 4, axis=1)\n",
    "  i = tf.sigmoid(i)\n",
    "  f = tf.sigmoid(f + 1.0)\n",
    "  o = tf.sigmoid(o)\n",
    "  u = tf.tanh(u)\n",
    "  c = f * c + i * u\n",
    "  h = o * tf.tanh(c)\n",
    "  return h, c\n",
    "\n",
    "\n",
    "def generator(input, batch_size, timesteps, encoder):\n",
    "  \"\"\"Generate batch with respect to input (a list). Encode its\n",
    "  strings to integers, returning an array of shape [batch_size, timesteps].\n",
    "  \"\"\"\n",
    "  while True:\n",
    "    imb = np.random.randint(0, len(input) - timesteps, batch_size)\n",
    "    encoded = np.asarray(\n",
    "        [[encoder[c] for c in input[i:(i + timesteps)]] for i in imb],\n",
    "        dtype=np.int32)\n",
    "    yield encoded\n",
    "\n",
    "\n",
    "def language_model(input):\n",
    "  \"\"\"Form p(x[0], ..., x[timesteps - 1]),\n",
    "\n",
    "  \\prod_{t=0}^{timesteps - 1} p(x[t] | x[:t]),\n",
    "\n",
    "  To calculate the probability, we call log_prob on\n",
    "  x = [x[0], ..., x[timesteps - 1]] given\n",
    "  `input` = [0, x[0], ..., x[timesteps - 2]].\n",
    "\n",
    "  We implement this separately from the generative model so the\n",
    "  forward pass, e.g., embedding/dense layers, can be parallelized.\n",
    "\n",
    "  [batch_size, timesteps] -> [batch_size, timesteps]\n",
    "  \"\"\"\n",
    "  x = tf.one_hot(input, depth=vocab_size, dtype=tf.float32)\n",
    "  #import pdb; pdb.set_trace()\n",
    "  h = tf.fill(tf.stack([tf.shape(x)[0], hidden_size]), 0.0)\n",
    "  c = tf.fill(tf.stack([tf.shape(x)[0], hidden_size]), 0.0)\n",
    "  hs = []\n",
    "  reuse = None\n",
    "  for t in range(timesteps):\n",
    "    if t > 0:\n",
    "      reuse = True\n",
    "    xt = x[:, t, :]\n",
    "    h, c = lstm_cell(xt, h, c, name=\"lstm\", reuse=reuse)\n",
    "    hs.append(h)\n",
    "\n",
    "  h = tf.stack(hs, 1)\n",
    "  logits = tf.layers.dense(h, vocab_size, name=\"dense\")\n",
    "  output = Categorical(logits=logits)\n",
    "  return output\n",
    "\n",
    "\n",
    "def language_model_gen(batch_size):\n",
    "  \"\"\"Generate x ~ prod p(x_t | x_{<t}). Output [batch_size, timesteps].\n",
    "  \"\"\"\n",
    "  # Initialize data input randomly.\n",
    "  x = tf.random_uniform([batch_size], 0, vocab_size, dtype=tf.int32)\n",
    "  h = tf.zeros([batch_size, hidden_size])\n",
    "  c = tf.zeros([batch_size, hidden_size])\n",
    "  xs = []\n",
    "  for _ in range(timesteps):\n",
    "    x = tf.one_hot(x, depth=vocab_size, dtype=tf.float32)\n",
    "    h, c = lstm_cell(x, h, c, name=\"lstm\")\n",
    "    logits = tf.layers.dense(h, vocab_size, name=\"dense\")\n",
    "    x = Categorical(logits=logits).value()\n",
    "    xs.append(x)\n",
    "\n",
    "  xs = tf.cast(tf.stack(xs, 1), tf.int32)\n",
    "  return xs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Seeding is not supported after initializing part of the graph. Please move set_seed to the beginning of your code.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ab20e3baf43c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0med\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/willtai/4thyearproject/edward/edward/edward/util/graphs.pyc\u001b[0m in \u001b[0;36mset_seed\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0mnode_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnode_names\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'keras_learning_phase'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     raise RuntimeError(\"Seeding is not supported after initializing \"\n\u001b[0m\u001b[1;32m     69\u001b[0m                        \u001b[0;34m\"part of the graph. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                        \"Please move set_seed to the beginning of your code.\")\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Seeding is not supported after initializing part of the graph. Please move set_seed to the beginning of your code."
     ]
    }
   ],
   "source": [
    "ed.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "x_train, _, x_test = text8(data_dir)\n",
    "vocab = string.ascii_lowercase + ' '\n",
    "vocab_size = len(vocab)\n",
    "# encoder = dict(zip(vocab, range(vocab_size)))\n",
    "# decoder = {v: k for k, v in encoder.items()}\n",
    "\n",
    "# data = generator(x_train, batch_size, timesteps, encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "\n",
    "SMILES_CHARS = [' ',\n",
    "                  '#', '%', '(', ')', '+', '-', '.', '/',\n",
    "                  '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                  '=', '@',\n",
    "                  'A', 'B', 'C', 'F', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P',\n",
    "                  'R', 'S', 'T', 'V', 'X', 'Z',\n",
    "                  '[', '\\\\', ']',\n",
    "                  'a', 'b', 'c', 'e', 'g', 'i', 'l', 'n', 'o', 'p', 'r', 's',\n",
    "                  't', 'u']\n",
    "smi2index = dict( (c,i) for i,c in enumerate( SMILES_CHARS ) )\n",
    "index2smi = dict( (i,c) for i,c in enumerate( SMILES_CHARS ) )\n",
    "def smiles_encoder( smiles, maxlen=120 ):\n",
    "    smiles = Chem.MolToSmiles(Chem.MolFromSmiles( smiles ))\n",
    "    X = np.zeros( ( maxlen, len( SMILES_CHARS ) ) )\n",
    "    for i, c in enumerate( smiles ):\n",
    "        X[i, smi2index[c] ] = 1\n",
    "    return X\n",
    " \n",
    "def smiles_decoder( X ):\n",
    "    smi = ''\n",
    "    X = X.argmax( axis=-1 )\n",
    "    for i in X:\n",
    "        smi += index2smi[ i ]\n",
    "    return smi\n",
    "\n",
    "encoder = smi2index\n",
    "decoder = index2smi\n",
    "data = generator(x_train, batch_size, timesteps, encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Task params {'target_name': 'measured log solubility in mols per litre', 'data_file': 'delaney.csv'}\n"
     ]
    }
   ],
   "source": [
    "import edward as ed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from edward.models import Normal\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "from neuralfingerprint import load_data\n",
    "from autograd import grad\n",
    "from keras.layers import Embedding, Dense\n",
    "\n",
    "task_params = {'target_name' : 'measured log solubility in mols per litre',\n",
    "               'data_file'   : 'delaney.csv'}\n",
    "N_train = 800\n",
    "N_val   = 20\n",
    "N_test  = 20\n",
    "\n",
    "model_params = dict(fp_length=50,    # Usually neural fps need far fewer dimensions than morgan.\n",
    "                    fp_depth=4,      # The depth of the network equals the fingerprint radius.\n",
    "                    conv_width=20,   # Only the neural fps need this parameter.\n",
    "                    h1_size=100,     # Size of hidden layer of network on top of fps.\n",
    "                    L2_reg=np.exp(-2))\n",
    "\n",
    "print(\"Loading data...\")\n",
    "traindata, valdata, testdata = load_data(\n",
    "    task_params['data_file'], (N_train, N_val, N_test),\n",
    "    input_name='smiles', target_name=task_params['target_name'])\n",
    "train_inputs, train_targets = traindata\n",
    "val_inputs,   val_targets   = valdata\n",
    "test_inputs,  test_targets  = testdata\n",
    "\n",
    "print(\"Task params\", task_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = train_inputs[0:10], test_inputs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable language_model/lstm/kernel/input already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-7-518778018d9b>\", line 9, in lstm_cell\n    initializer=tf.orthogonal_initializer(1.0))\n  File \"<ipython-input-7-518778018d9b>\", line 64, in language_model\n    h, c = lstm_cell(xt, h, c, name=\"lstm\", reuse=reuse)\n  File \"<ipython-input-11-60e8d4d02114>\", line 6, in <module>\n    x = language_model(x_ph_shift)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-60e8d4d02114>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# Shift input sequence to right by 1, [0, x[0], ..., x[timesteps - 2]].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mx_ph_shift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguage_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ph_shift\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"language_model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-518778018d9b>\u001b[0m in \u001b[0;36mlanguage_model\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0mreuse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lstm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-518778018d9b>\u001b[0m in \u001b[0;36mlstm_cell\u001b[0;34m(x, h, c, name, reuse)\u001b[0m\n\u001b[1;32m      7\u001b[0m     wx = tf.get_variable(\"kernel/input\", [nin, nout * 4],\n\u001b[1;32m      8\u001b[0m                          \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                          initializer=tf.orthogonal_initializer(1.0))\n\u001b[0m\u001b[1;32m     10\u001b[0m     wh = tf.get_variable(\"kernel/hidden\", [nout, nout * 4],\n\u001b[1;32m     11\u001b[0m                          \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1201\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1204\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1205\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1090\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    740\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 742\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    743\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable language_model/lstm/kernel/input already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-7-518778018d9b>\", line 9, in lstm_cell\n    initializer=tf.orthogonal_initializer(1.0))\n  File \"<ipython-input-7-518778018d9b>\", line 64, in language_model\n    h, c = lstm_cell(xt, h, c, name=\"lstm\", reuse=reuse)\n  File \"<ipython-input-11-60e8d4d02114>\", line 6, in <module>\n    x = language_model(x_ph_shift)\n"
     ]
    }
   ],
   "source": [
    "# MODEL\n",
    "x_ph = tf.placeholder(tf.int32, [None, timesteps])\n",
    "with tf.variable_scope(\"language_model\"):\n",
    "  # Shift input sequence to right by 1, [0, x[0], ..., x[timesteps - 2]].\n",
    "  x_ph_shift = tf.pad(x_ph, [[0, 0], [1, 0]])[:, :-1]\n",
    "  x = language_model(x_ph_shift)\n",
    "\n",
    "with tf.variable_scope(\"language_model\", reuse=True):\n",
    "  x_gen = language_model_gen(5)\n",
    "\n",
    "imb = range(0, len(x_test) - timesteps, timesteps)\n",
    "encoded_x_test = np.asarray(\n",
    "    [[encoder[c] for c in x_test[i:(i + timesteps)]] for i in imb],\n",
    "    dtype=np.int32)\n",
    "test_size = encoded_x_test.shape[0]\n",
    "print(\"Test set shape: {}\".format(encoded_x_test.shape))\n",
    "test_nll = -tf.reduce_sum(x.log_prob(x_ph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sets of parameters: 5\n",
      "Number of parameters: 1119771\n",
      "<tf.Variable 'language_model/lstm/kernel/input:0' shape=(27, 2048) dtype=float32_ref>\n",
      "<tf.Variable 'language_model/lstm/kernel/hidden:0' shape=(512, 2048) dtype=float32_ref>\n",
      "<tf.Variable 'language_model/lstm/bias:0' shape=(2048,) dtype=float32_ref>\n",
      "<tf.Variable 'language_model/dense/kernel:0' shape=(512, 27) dtype=float32_ref>\n",
      "<tf.Variable 'language_model/dense/bias:0' shape=(27,) dtype=float32_ref>\n",
      "Epoch: 0.5\n",
      "Train average bits/char: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder_1' with dtype int32 and shape [?,64]\n\t [[Node: Placeholder_1 = Placeholder[dtype=DT_INT32, shape=[?,64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op u'Placeholder_1', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-60e8d4d02114>\", line 2, in <module>\n    x_ph = tf.placeholder(tf.int32, [None, timesteps])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 1599, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3091, in _placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_1' with dtype int32 and shape [?,64]\n\t [[Node: Placeholder_1 = Placeholder[dtype=DT_INT32, shape=[?,64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-dcd4551363f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_x_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mavg_nll\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_nll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0mavg_nll\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_1' with dtype int32 and shape [?,64]\n\t [[Node: Placeholder_1 = Placeholder[dtype=DT_INT32, shape=[?,64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op u'Placeholder_1', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-60e8d4d02114>\", line 2, in <module>\n    x_ph = tf.placeholder(tf.int32, [None, timesteps])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 1599, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3091, in _placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_1' with dtype int32 and shape [?,64]\n\t [[Node: Placeholder_1 = Placeholder[dtype=DT_INT32, shape=[?,64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "inference = ed.MAP({}, {x: x_ph})\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "inference.initialize(optimizer=optimizer, logdir=log_dir, log_timestamp=False)\n",
    "\n",
    "print(\"Number of sets of parameters: {}\".format(len(tf.trainable_variables())))\n",
    "print(\"Number of parameters: {}\".format(\n",
    "    np.sum([np.prod(v.shape.as_list()) for v in tf.trainable_variables()])))\n",
    "for v in tf.trainable_variables():\n",
    "  print(v)\n",
    "\n",
    "sess = ed.get_session()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Double n_epoch and print progress every half an epoch.\n",
    "n_iter_per_epoch = len(x_train) // (batch_size * timesteps * 2)\n",
    "epoch = 0.0\n",
    "for _ in range(n_epoch * 2):\n",
    "  epoch += 0.5\n",
    "  print(\"Epoch: {0}\".format(epoch))\n",
    "  avg_nll = 0.0\n",
    "\n",
    "  pbar = Progbar(n_iter_per_epoch)\n",
    "  for t in range(1, n_iter_per_epoch + 1):\n",
    "    import pdb; pdb.set_trace()\n",
    "    pbar.update(t)\n",
    "    x_batch = next(data)\n",
    "    info_dict = inference.update({x_ph: x_batch})\n",
    "    avg_nll += info_dict['loss']\n",
    "\n",
    "  # Print average bits per character over epoch.\n",
    "  avg_nll /= (n_iter_per_epoch * batch_size * timesteps * np.log(2))\n",
    "  print(\"Train average bits/char: {:0.8f}\".format(avg_nll))\n",
    "\n",
    "  # Print per-data point log-likelihood on test set.\n",
    "  avg_nll = 0.0\n",
    "  for start in range(0, test_size, batch_size):\n",
    "    end = min(test_size, start + batch_size)\n",
    "    x_batch = encoded_x_test[start:end]\n",
    "    avg_nll += sess.run(test_nll, {x_ph: x_batch})\n",
    "\n",
    "  avg_nll /= test_size\n",
    "  print(\"Test average NLL: {:0.8f}\".format(avg_nll))\n",
    "\n",
    "  # Generate samples from model.\n",
    "  samples = sess.run(x_gen)\n",
    "  samples = [''.join([decoder[c] for c in sample]) for sample in samples]\n",
    "  print(\"Samples:\")\n",
    "  for sample in samples:\n",
    "    print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

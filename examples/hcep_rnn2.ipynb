{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Task params {'target_name': 'measured log solubility in mols per litre', 'data_file': 'delaney.csv'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example regression script using neural fingerprints.\n",
    "#\n",
    "# Compares Morgan fingerprints to neural fingerprints.\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "from neuralfingerprint import load_data\n",
    "from neuralfingerprint import build_morgan_deep_net\n",
    "from neuralfingerprint import build_conv_deep_net\n",
    "from neuralfingerprint import normalize_array, adam\n",
    "from neuralfingerprint import build_batched_grad\n",
    "from neuralfingerprint.util import rmse\n",
    "\n",
    "from autograd import grad\n",
    "\n",
    "task_params = {'target_name' : 'measured log solubility in mols per litre',\n",
    "               'data_file'   : 'delaney.csv'}\n",
    "N_train = 800\n",
    "N_val   = 20\n",
    "N_test  = 20\n",
    "\n",
    "model_params = dict(fp_length=50,    # Usually neural fps need far fewer dimensions than morgan.\n",
    "                    fp_depth=4,      # The depth of the network equals the fingerprint radius.\n",
    "                    conv_width=20,   # Only the neural fps need this parameter.\n",
    "                    h1_size=100,     # Size of hidden layer of network on top of fps.\n",
    "                    L2_reg=np.exp(-2))\n",
    "train_params = dict(num_iters=100,\n",
    "                    batch_size=100,\n",
    "                    init_scale=np.exp(-4),\n",
    "                    step_size=np.exp(-6))\n",
    "\n",
    "# Define the architecture of the network that sits on top of the fingerprints.\n",
    "vanilla_net_params = dict(\n",
    "    layer_sizes = [model_params['fp_length'], model_params['h1_size']],  # One hidden layer.\n",
    "    normalize=True, L2_reg = model_params['L2_reg'], nll_func = rmse)\n",
    "\n",
    "def train_nn(pred_fun, loss_fun, num_weights, train_smiles, train_raw_targets, train_params, seed=0,\n",
    "             validation_smiles=None, validation_raw_targets=None):\n",
    "    \"\"\"loss_fun has inputs (weights, smiles, targets)\"\"\"\n",
    "    print \"Total number of weights in the network:\", num_weights\n",
    "    init_weights = npr.RandomState(seed).randn(num_weights) * train_params['init_scale']\n",
    "\n",
    "    num_print_examples = 100\n",
    "    train_targets, undo_norm = normalize_array(train_raw_targets)\n",
    "    training_curve = []\n",
    "    def callback(weights, iter):\n",
    "        if iter % 10 == 0:\n",
    "            print \"max of weights\", np.max(np.abs(weights))\n",
    "            train_preds = undo_norm(pred_fun(weights, train_smiles[:num_print_examples]))\n",
    "            cur_loss = loss_fun(weights, train_smiles[:num_print_examples], train_targets[:num_print_examples])\n",
    "            training_curve.append(cur_loss)\n",
    "            print \"Iteration\", iter, \"loss\", cur_loss,\\\n",
    "                  \"train RMSE\", rmse(train_preds, train_raw_targets[:num_print_examples])\n",
    "            if validation_smiles is not None:\n",
    "                validation_preds = undo_norm(pred_fun(weights, validation_smiles))\n",
    "                print \"Validation RMSE\", iter, \":\", rmse(validation_preds, validation_raw_targets)\n",
    "\n",
    "    # Build gradient using autograd.\n",
    "    grad_fun = grad(loss_fun)\n",
    "    grad_fun_with_data = build_batched_grad(grad_fun, train_params['batch_size'],\n",
    "                                            train_smiles, train_targets)\n",
    "\n",
    "    # Optimize weights.\n",
    "    trained_weights = adam(grad_fun_with_data, init_weights, callback=callback,\n",
    "                           num_iters=train_params['num_iters'], step_size=train_params['step_size'])\n",
    "\n",
    "    def predict_func(new_smiles):\n",
    "        \"\"\"Returns to the original units that the raw targets were in.\"\"\"\n",
    "        return undo_norm(pred_fun(trained_weights, new_smiles))\n",
    "    return predict_func, trained_weights, training_curve\n",
    "\n",
    "\n",
    "\n",
    "print \"Loading data...\"\n",
    "traindata, valdata, testdata = load_data(\n",
    "    task_params['data_file'], (N_train, N_val, N_test),\n",
    "    input_name='smiles', target_name=task_params['target_name'])\n",
    "train_inputs, train_targets = traindata\n",
    "val_inputs,   val_targets   = valdata\n",
    "test_inputs,  test_targets  = testdata\n",
    "\n",
    "\n",
    "def print_performance(pred_func):\n",
    "    train_preds = pred_func(train_inputs)\n",
    "    val_preds = pred_func(val_inputs)\n",
    "    print \"\\nPerformance (RMSE) on \" + task_params['target_name'] + \":\"\n",
    "    print \"Train:\", rmse(train_preds, train_targets)\n",
    "    print \"Test: \", rmse(val_preds,  val_targets)\n",
    "    print \"-\" * 80\n",
    "    return rmse(val_preds, val_targets)\n",
    "\n",
    "def run_morgan_experiment():\n",
    "    loss_fun, pred_fun, net_parser = \\\n",
    "        build_morgan_deep_net(model_params['fp_length'],\n",
    "                              model_params['fp_depth'], vanilla_net_params)\n",
    "    num_weights = len(net_parser)\n",
    "    predict_func, trained_weights, conv_training_curve = \\\n",
    "        train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n",
    "                 train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n",
    "    return print_performance(predict_func)\n",
    "\n",
    "def run_conv_experiment():\n",
    "    conv_layer_sizes = [model_params['conv_width']] * model_params['fp_depth']\n",
    "    conv_arch_params = {'num_hidden_features' : conv_layer_sizes,\n",
    "                        'fp_length' : model_params['fp_length'], 'normalize' : 1}\n",
    "    loss_fun, pred_fun, conv_parser = \\\n",
    "        build_conv_deep_net(conv_arch_params, vanilla_net_params, model_params['L2_reg'])\n",
    "    num_weights = len(conv_parser)\n",
    "    predict_func, trained_weights, conv_training_curve = \\\n",
    "        train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n",
    "                 train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n",
    "    test_predictions = predict_func(test_inputs)\n",
    "    return rmse(test_predictions, test_targets)\n",
    "\n",
    "print \"Task params\", task_params\n",
    "print\n",
    "#     print \"Starting Morgan fingerprint experiment...\"\n",
    "#     test_loss_morgan = run_morgan_experiment()\n",
    "#     print \"Starting neural fingerprint experiment...\"\n",
    "#     test_loss_neural = run_conv_experiment()\n",
    "#     print\n",
    "#print \"Morgan test RMSE:\", test_loss_morgan, \"Neural test RMSE:\", test_loss_neural\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data : \n",
      "Cc1occc1C(=O)Nc2ccccc2\n"
     ]
    }
   ],
   "source": [
    "print \"Training data : \" \n",
    "print train_inputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L2_reg': 0.1353352832366127,\n",
       " 'conv_width': 20,\n",
       " 'fp_depth': 4,\n",
       " 'fp_length': 50,\n",
       " 'h1_size': 100}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edward inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edward as ed\n",
    "import tensorflow as tf\n",
    "from edward.models import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 50  # number of hidden units\n",
    "D = 10  # number of features\n",
    "lr = 5e-03\n",
    "\n",
    "x_train = train_inputs[0:10]\n",
    "y_train = np.reshape(train_targets[0:10],[len(x_train),1])\n",
    "\n",
    "N = len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell(hprev, xt):\n",
    "  return tf.tanh(ed.dot(hprev, Wh) + ed.dot(xt, Wx) + bh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"model\"):\n",
    "    Wh = Normal(loc=tf.zeros([H, H]), scale=tf.ones([H, H]), name=\"Wh\")\n",
    "    Wx = Normal(loc=tf.zeros([D, H]), scale=tf.ones([D, H]), name=\"Wx\")\n",
    "    Wy = Normal(loc=tf.zeros([H, 1]), scale=tf.ones([H, 1]), name=\"Wy\")\n",
    "    bh = Normal(loc=tf.zeros(H), scale=tf.ones(H), name=\"bh\")\n",
    "    by = Normal(loc=tf.zeros(1), scale=tf.ones(1), name=\"by\")\n",
    "    \n",
    "with tf.name_scope(\"posterior\"):\n",
    "  with tf.name_scope(\"qWh\"):\n",
    "    qWh = Normal(loc=tf.Variable(tf.random_normal([H, H]), name=\"loc\"),\n",
    "                  scale=tf.nn.softplus(\n",
    "                      tf.Variable(tf.random_normal([H, H]), name=\"scale\")))\n",
    "  with tf.name_scope(\"qWx\"):\n",
    "    qWx = Normal(loc=tf.Variable(tf.random_normal([D, H]), name=\"loc\"),\n",
    "                  scale=tf.nn.softplus(\n",
    "                      tf.Variable(tf.random_normal([D, H]), name=\"scale\")))\n",
    "  with tf.name_scope(\"qWy\"):\n",
    "    qWy = Normal(loc=tf.Variable(tf.random_normal([H, 1]), name=\"loc\"),\n",
    "                  scale=tf.nn.softplus(\n",
    "                      tf.Variable(tf.random_normal([H, 1]), name=\"scale\")))\n",
    "  with tf.name_scope(\"qbh\"):\n",
    "    qbh = Normal(loc=tf.Variable(tf.random_normal([H]), name=\"loc\"),\n",
    "                  scale=tf.nn.softplus(\n",
    "                      tf.Variable(tf.random_normal([H]), name=\"scale\")))\n",
    "  with tf.name_scope(\"qby\"):\n",
    "    qby = Normal(loc=tf.Variable(tf.random_normal([1]), name=\"loc\"),\n",
    "                  scale=tf.nn.softplus(\n",
    "                      tf.Variable(tf.random_normal([1]), name=\"scale\")))\n",
    "\n",
    "  x = tf.placeholder(tf.float32, [N, None])\n",
    "  h = tf.scan(rnn_cell, x, initializer=tf.zeros(H))\n",
    "  y = Normal(loc=tf.matmul(h, Wy) + by, scale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/willtai/4thyearproject/edward/edward/edward/util/random_variables.py:50: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  not np.issubdtype(value.dtype, np.float) and \\\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'outer_context'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e5d06666adf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'log'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/willtai/4thyearproject/edward/edward/edward/inferences/inference.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, variables, use_coordinator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mPassed\u001b[0m \u001b[0minto\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \"\"\"\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/willtai/4thyearproject/edward/edward/edward/inferences/klqp.pyc\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self, n_samples, kl_scaling, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_scaling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkl_scaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKLqp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbuild_loss_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/willtai/4thyearproject/edward/edward/edward/inferences/variational_inference.pyc\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self, optimizer, var_list, use_prettytensor, global_step, *args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_loss_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/willtai/4thyearproject/edward/edward/edward/inferences/klqp.pyc\u001b[0m in \u001b[0;36mbuild_loss_and_gradients\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_reparameterizable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mis_analytic_kl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuild_reparam_kl_loss_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m       \u001b[0;31m# elif is_analytic_entropy:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m       \u001b[0;31m#    return build_reparam_entropy_loss_and_gradients(self, var_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/willtai/4thyearproject/edward/edward/edward/inferences/klqp.pyc\u001b[0m in \u001b[0;36mbuild_reparam_kl_loss_and_gradients\u001b[0;34m(inference, var_list)\u001b[0m\n\u001b[1;32m    711\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_log_lik\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mkl_penalty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m   \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m   \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.pyc\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\u001b[0m\n\u001b[1;32m    488\u001b[0m     pending_count, loop_state = _PendingCount(ops.get_default_graph(), to_ops,\n\u001b[1;32m    489\u001b[0m                                               \u001b[0mfrom_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m                                               colocate_gradients_with_ops)\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;31m# Iterate over the collected ops.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.pyc\u001b[0m in \u001b[0;36m_PendingCount\u001b[0;34m(graph, to_ops, from_ops, colocate_gradients_with_ops)\u001b[0m\n\u001b[1;32m    189\u001b[0m   \u001b[0;31m# 'loop_state' is None if there are no while loops.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m   loop_state = control_flow_ops.MaybeCreateControlFlowState(\n\u001b[0;32m--> 191\u001b[0;31m       between_op_list, between_ops, colocate_gradients_with_ops)\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m   \u001b[0;31m# Initialize pending count for between ops.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36mMaybeCreateControlFlowState\u001b[0;34m(between_op_list, between_ops, colocate_gradients_with_ops)\u001b[0m\n\u001b[1;32m   1308\u001b[0m           \u001b[0mloop_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAddWhileContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetween_op_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetween_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m         \u001b[0mloop_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAddWhileContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetween_op_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetween_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mloop_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36mAddWhileContext\u001b[0;34m(self, op, between_op_list, between_ops)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgrad_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m       \u001b[0;31m# This is a new while loop so create a grad state for it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m       \u001b[0mouter_forward_ctxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_ctxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mouter_forward_ctxt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0mouter_forward_ctxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mouter_forward_ctxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetWhileContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'outer_context'"
     ]
    }
   ],
   "source": [
    "inference = ed.KLqp({Wh: qWh, bh: qbh,\n",
    "                     Wx: qWx,\n",
    "                     Wy: qWy, by: qby}, data={x: x_train, y: y_train})\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "inference.run(logdir='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
